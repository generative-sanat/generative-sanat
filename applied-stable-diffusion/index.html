<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Mert Cobanov" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Applied Stable Diffusion - Generative Sanat</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Applied Stable Diffusion";
        var mkdocs_page_input_path = "applied-stable-diffusion.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Generative Sanat
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Hello</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../ldm/">What is Latent Diffusion</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../vit/">ViT Visual Transformers</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../vae/">VAE</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../scheduler/">Scheduler</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../sd/">Stable Diffusion</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Applied Stable Diffusion</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#components-of-stable-diffusion-model">Components of Stable Diffusion Model</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#text-preperation">Text Preperation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#prep-latents">Prep latents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#loop">Loop</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#display">Display</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="" href="../samplers.md">Samplers</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="" href="../prompting.md">Prompting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dream-booth/">Dream Booth</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cross-attention/">Cross-attention</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../glossary/">Glossary</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Generative Sanat</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li>
      <li>Applied Stable Diffusion</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="applied-stable-diffuion">Applied Stable Diffuion</h1>
<h2 id="components-of-stable-diffusion-model">Components of Stable Diffusion Model</h2>
<ul>
<li>VAE</li>
<li>Tokenizer, Text Encoder</li>
<li>UNet</li>
<li>Scheduler</li>
</ul>
<pre><code class="language-python"># Load the autoencoder model which will be used to decode the latents into image space. 
vae = AutoencoderKL.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, subfolder=&quot;vae&quot;)

# Load the tokenizer and text encoder to tokenize and encode the text. 
tokenizer = CLIPTokenizer.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;)
text_encoder = CLIPTextModel.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;)

# The UNet model for generating the latents.
unet = UNet2DConditionModel.from_pretrained(&quot;CompVis/stable-diffusion-v1-4&quot;, subfolder=&quot;unet&quot;)

# The noise scheduler
scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=&quot;scaled_linear&quot;, num_train_timesteps=1000)

# To the GPU we go!
vae = vae.to(torch_device)
text_encoder = text_encoder.to(torch_device)
unet = unet.to(torch_device);
</code></pre>
<h2 id="text-preperation">Text Preperation</h2>
<p>prompt = ["A watercolor painting of an otter"]</p>
<pre><code class="language-python">text_input = tokenizer(prompt, padding=&quot;max_length&quot;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>Tokenize edilmis text inputu inceleyelim</p>
<pre><code class="language-python">text_input.keys()
# dict_keys(['input_ids', 'attention_mask'])

text_input['input_ids'].shape
# torch.Size([1, 77])

text_input['attention_mask']
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
</code></pre>
<p>Uncondition ve condition embeddingleri concat ediyoruz, Daha sonra bu bolum CFG icin kullanilacak.</p>
<pre><code class="language-python">batch_size = 1

with torch.no_grad():
    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]
max_length = text_input.input_ids.shape[-1]
uncond_input = tokenizer(
    [&quot;&quot;] * batch_size, padding=&quot;max_length&quot;, max_length=max_length, return_tensors=&quot;pt&quot;
)

with torch.no_grad():
    uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] 
text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
</code></pre>
<p>Goruldugu gibi 77x768 boyutunde iki matrisimiz var.</p>
<pre><code class="language-python">text_embeddings.shape
# torch.Size([2, 77, 768])
</code></pre>
<p>30 adim denoising yapacagimizi dusunelim.</p>
<pre><code class="language-python"># Prep Scheduler
num_inference_steps = 30 # Number of denoising steps
scheduler.set_timesteps(num_inference_steps)
</code></pre>
<p>Latentlari olusturalim, stable diffusion 1.x modeli 512x512 boyutlu fotograflarla egitildi bu yuzden biz de genislik ve yuksekligi 512x512 olarak ayarliyoruz. Hatirlarsaniz unet modeli icin goruntu represantasyonlarini 4x64x64 boyutuna indirgeyecektik, bu yuzden latent matrislerimizdeki boyutu 8'e boluyoruz.</p>
<h2 id="prep-latents">Prep latents</h2>
<pre><code class="language-python">
height = 512                        # default height of Stable Diffusion
width = 512                         # default width of Stable Diffusion
generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise

latents = torch.randn(
  (batch_size, unet.in_channels, height // 8, width // 8),
  generator=generator,
)

latents = latents.to(torch_device)
latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]

</code></pre>
<pre><code class="language-python">latents.shape
# torch.Size([1, 4, 64, 64])
</code></pre>
<h2 id="loop">Loop</h2>
<pre><code class="language-python"># Loop
with autocast(&quot;cuda&quot;):
    for i, t in tqdm(enumerate(scheduler.timesteps)):
        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
        latent_model_input = torch.cat([latents] * 2)
        sigma = scheduler.sigmas[i]
        # Scale the latents (preconditioning):
        # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below
        latent_model_input = scheduler.scale_model_input(latent_model_input, t)

        # predict the noise residual
        with torch.no_grad():
            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # perform guidance
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        # compute the previous noisy sample x_t -&gt; x_t-1
        # latents = scheduler.step(noise_pred, i, latents)[&quot;prev_sample&quot;] # Diffusers 0.3 and below
        latents = scheduler.step(noise_pred, t, latents).prev_sample

# scale and decode the image latents with vae
latents = 1 / 0.18215 * latents
with torch.no_grad():
    image = vae.decode(latents).sample
</code></pre>
<h2 id="display">Display</h2>
<pre><code class="language-python">image = (image / 2 + 0.5).clamp(0, 1)
image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
images = (image * 255).round().astype(&quot;uint8&quot;)
pil_images = [Image.fromarray(image) for image in images]
pil_images[0]
</code></pre>
<p><img alt="fox" src="../assets/fox.png" /></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../sd/" class="btn btn-neutral float-left" title="Stable Diffusion"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../dream-booth/" class="btn btn-neutral float-right" title="Dream Booth">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../sd/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../dream-booth/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
