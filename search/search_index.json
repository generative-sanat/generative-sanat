{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Generative Sanat","title":"Hello"},{"location":"#welcome-to-generative-sanat","text":"","title":"Welcome to Generative Sanat"},{"location":"applied-stable-diffusion/","text":"Applied Stable Diffuion Components of Stable Diffusion Model VAE Tokenizer, Text Encoder UNet Scheduler # Load the autoencoder model which will be used to decode the latents into image space. vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\") # Load the tokenizer and text encoder to tokenize and encode the text. tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\") text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\") # The UNet model for generating the latents. unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\") # The noise scheduler scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000) # To the GPU we go! vae = vae.to(torch_device) text_encoder = text_encoder.to(torch_device) unet = unet.to(torch_device); Text Preperation prompt = [\"A watercolor painting of an otter\"] text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") Tokenize edilmis text inputu inceleyelim text_input.keys() # dict_keys(['input_ids', 'attention_mask']) text_input['input_ids'].shape # torch.Size([1, 77]) text_input['attention_mask'] # tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) Uncondition ve condition embeddingleri concat ediyoruz, Daha sonra bu bolum CFG icin kullanilacak. batch_size = 1 with torch.no_grad(): text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0] max_length = text_input.input_ids.shape[-1] uncond_input = tokenizer( [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\" ) with torch.no_grad(): uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] text_embeddings = torch.cat([uncond_embeddings, text_embeddings]) Goruldugu gibi 77x768 boyutunde iki matrisimiz var. text_embeddings.shape # torch.Size([2, 77, 768]) 30 adim denoising yapacagimizi dusunelim. # Prep Scheduler num_inference_steps = 30 # Number of denoising steps scheduler.set_timesteps(num_inference_steps) Latentlari olusturalim, stable diffusion 1.x modeli 512x512 boyutlu fotograflarla egitildi bu yuzden biz de genislik ve yuksekligi 512x512 olarak ayarliyoruz. Hatirlarsaniz unet modeli icin goruntu represantasyonlarini 4x64x64 boyutuna indirgeyecektik, bu yuzden latent matrislerimizdeki boyutu 8'e boluyoruz. Prep latents height = 512 # default height of Stable Diffusion width = 512 # default width of Stable Diffusion generator = torch.manual_seed(32) # Seed generator to create the inital latent noise latents = torch.randn( (batch_size, unet.in_channels, height // 8, width // 8), generator=generator, ) latents = latents.to(torch_device) latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0] latents.shape # torch.Size([1, 4, 64, 64]) Loop # Loop with autocast(\"cuda\"): for i, t in tqdm(enumerate(scheduler.timesteps)): # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes. latent_model_input = torch.cat([latents] * 2) sigma = scheduler.sigmas[i] # Scale the latents (preconditioning): # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below latent_model_input = scheduler.scale_model_input(latent_model_input, t) # predict the noise residual with torch.no_grad(): noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -> x_t-1 # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below latents = scheduler.step(noise_pred, t, latents).prev_sample # scale and decode the image latents with vae latents = 1 / 0.18215 * latents with torch.no_grad(): image = vae.decode(latents).sample Display image = (image / 2 + 0.5).clamp(0, 1) image = image.detach().cpu().permute(0, 2, 3, 1).numpy() images = (image * 255).round().astype(\"uint8\") pil_images = [Image.fromarray(image) for image in images] pil_images[0]","title":"Applied Stable Diffusion"},{"location":"applied-stable-diffusion/#applied-stable-diffuion","text":"","title":"Applied Stable Diffuion"},{"location":"applied-stable-diffusion/#components-of-stable-diffusion-model","text":"VAE Tokenizer, Text Encoder UNet Scheduler # Load the autoencoder model which will be used to decode the latents into image space. vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\") # Load the tokenizer and text encoder to tokenize and encode the text. tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\") text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\") # The UNet model for generating the latents. unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\") # The noise scheduler scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000) # To the GPU we go! vae = vae.to(torch_device) text_encoder = text_encoder.to(torch_device) unet = unet.to(torch_device);","title":"Components of Stable Diffusion Model"},{"location":"applied-stable-diffusion/#text-preperation","text":"prompt = [\"A watercolor painting of an otter\"] text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") Tokenize edilmis text inputu inceleyelim text_input.keys() # dict_keys(['input_ids', 'attention_mask']) text_input['input_ids'].shape # torch.Size([1, 77]) text_input['attention_mask'] # tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) Uncondition ve condition embeddingleri concat ediyoruz, Daha sonra bu bolum CFG icin kullanilacak. batch_size = 1 with torch.no_grad(): text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0] max_length = text_input.input_ids.shape[-1] uncond_input = tokenizer( [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\" ) with torch.no_grad(): uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] text_embeddings = torch.cat([uncond_embeddings, text_embeddings]) Goruldugu gibi 77x768 boyutunde iki matrisimiz var. text_embeddings.shape # torch.Size([2, 77, 768]) 30 adim denoising yapacagimizi dusunelim. # Prep Scheduler num_inference_steps = 30 # Number of denoising steps scheduler.set_timesteps(num_inference_steps) Latentlari olusturalim, stable diffusion 1.x modeli 512x512 boyutlu fotograflarla egitildi bu yuzden biz de genislik ve yuksekligi 512x512 olarak ayarliyoruz. Hatirlarsaniz unet modeli icin goruntu represantasyonlarini 4x64x64 boyutuna indirgeyecektik, bu yuzden latent matrislerimizdeki boyutu 8'e boluyoruz.","title":"Text Preperation"},{"location":"applied-stable-diffusion/#prep-latents","text":"height = 512 # default height of Stable Diffusion width = 512 # default width of Stable Diffusion generator = torch.manual_seed(32) # Seed generator to create the inital latent noise latents = torch.randn( (batch_size, unet.in_channels, height // 8, width // 8), generator=generator, ) latents = latents.to(torch_device) latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0] latents.shape # torch.Size([1, 4, 64, 64])","title":"Prep latents"},{"location":"applied-stable-diffusion/#loop","text":"# Loop with autocast(\"cuda\"): for i, t in tqdm(enumerate(scheduler.timesteps)): # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes. latent_model_input = torch.cat([latents] * 2) sigma = scheduler.sigmas[i] # Scale the latents (preconditioning): # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below latent_model_input = scheduler.scale_model_input(latent_model_input, t) # predict the noise residual with torch.no_grad(): noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -> x_t-1 # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below latents = scheduler.step(noise_pred, t, latents).prev_sample # scale and decode the image latents with vae latents = 1 / 0.18215 * latents with torch.no_grad(): image = vae.decode(latents).sample","title":"Loop"},{"location":"applied-stable-diffusion/#display","text":"image = (image / 2 + 0.5).clamp(0, 1) image = image.detach().cpu().permute(0, 2, 3, 1).numpy() images = (image * 255).round().astype(\"uint8\") pil_images = [Image.fromarray(image) for image in images] pil_images[0]","title":"Display"},{"location":"cross-attention/","text":"","title":"Cross-attention"},{"location":"dream-booth/","text":"Dreambooth","title":"Dream Booth"},{"location":"dream-booth/#dreambooth","text":"","title":"Dreambooth"},{"location":"glossary/","text":"Glossary Noise UNet GroupNorm applies group normalization to the inputs of each block Dropout layers for smoother training Multiple resnet layers per block (if layers_per_block isn't set to 1) Attention (usually used only at lower resolution blocks) Conditioning on the timestep. Downsampling and upsampling blocks with learnable parameters fp16 (half precision) In computing, half precision (sometimes called FP16) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory. It is intended for storage of floating-point values in applications where higher precision is not essential, in particular image processing and neural networks. DDPM Schedulers or Samplers Sources https://github.com/huggingface/diffusion-models-class/tree/main/unit1","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#noise","text":"","title":"Noise"},{"location":"glossary/#unet","text":"GroupNorm applies group normalization to the inputs of each block Dropout layers for smoother training Multiple resnet layers per block (if layers_per_block isn't set to 1) Attention (usually used only at lower resolution blocks) Conditioning on the timestep. Downsampling and upsampling blocks with learnable parameters","title":"UNet"},{"location":"glossary/#fp16-half-precision","text":"In computing, half precision (sometimes called FP16) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory. It is intended for storage of floating-point values in applications where higher precision is not essential, in particular image processing and neural networks.","title":"fp16 (half precision)"},{"location":"glossary/#ddpm","text":"","title":"DDPM"},{"location":"glossary/#schedulers-or-samplers","text":"","title":"Schedulers or Samplers"},{"location":"glossary/#sources","text":"https://github.com/huggingface/diffusion-models-class/tree/main/unit1","title":"Sources"},{"location":"ldm/","text":"Latent Diffusion Models Recent advances in machine learning and artificial intelligence have made it possible to generate high-resolution images using latent diffusion models. These models are a type of mathematical model that is used to study the spread of knowledge, ideas, or innovations through a population or social network. In the context of image synthesis, these models can be used to generate photorealistic images from low-resolution inputs. One of the key challenges in generating high-resolution images with latent diffusion models is the so-called \"resolution gap.\" This refers to the difference between the resolution of the input image and the desired output image. In order to generate a high-resolution output image from a low-resolution input image, the latent diffusion model must be able to fill in the missing details and add new features to the image. Recent research has demonstrated that latent diffusion models can be trained to perform high-resolution image synthesis using a process called \"progressive growing.\" In this process, the latent diffusion model is trained on a set of low-resolution images and then gradually increased in resolution as it generates high-resolution outputs. This allows the model to learn the finer details of the images and generate more realistic outputs. One of the key advantages of using latent diffusion models for image synthesis is that they can generate high-resolution images that are consistent with the input image. This means that the generated images will have the same style and composition as the input image, but with added details and features. This can be useful for applications such as image super-resolution, inpainting, and stylization. Overall, the use of latent diffusion models for high-resolution image synthesis is a promising area of research that has the potential to revolutionize the field of computer graphics and image processing. By leveraging the power of machine learning and AI, these models can generate photorealistic images from low-resolution inputs, and open up new possibilities for image manipulation and generation.","title":"What is Latent Diffusion"},{"location":"ldm/#latent-diffusion-models","text":"Recent advances in machine learning and artificial intelligence have made it possible to generate high-resolution images using latent diffusion models. These models are a type of mathematical model that is used to study the spread of knowledge, ideas, or innovations through a population or social network. In the context of image synthesis, these models can be used to generate photorealistic images from low-resolution inputs. One of the key challenges in generating high-resolution images with latent diffusion models is the so-called \"resolution gap.\" This refers to the difference between the resolution of the input image and the desired output image. In order to generate a high-resolution output image from a low-resolution input image, the latent diffusion model must be able to fill in the missing details and add new features to the image. Recent research has demonstrated that latent diffusion models can be trained to perform high-resolution image synthesis using a process called \"progressive growing.\" In this process, the latent diffusion model is trained on a set of low-resolution images and then gradually increased in resolution as it generates high-resolution outputs. This allows the model to learn the finer details of the images and generate more realistic outputs. One of the key advantages of using latent diffusion models for image synthesis is that they can generate high-resolution images that are consistent with the input image. This means that the generated images will have the same style and composition as the input image, but with added details and features. This can be useful for applications such as image super-resolution, inpainting, and stylization. Overall, the use of latent diffusion models for high-resolution image synthesis is a promising area of research that has the potential to revolutionize the field of computer graphics and image processing. By leveraging the power of machine learning and AI, these models can generate photorealistic images from low-resolution inputs, and open up new possibilities for image manipulation and generation.","title":"Latent Diffusion Models"},{"location":"scheduler/","text":"Scheduler Scheduler, sampler ayni anlamlarda kullaniliyor. Diffusion prosesinde ana amacimiz fotografa yavas yavas gurultu eklemek ve her adimda modelin bu gurultuyu nasil kaldiracagini ogrenmesini beklemek. Modelin inference asamasinda verdigimiz random noise inputu kaldirarak goruntu olusturmasini istiyoruz. Goruntulere gurultu ekleme sureci lineer olmayan bir sekilde gerceklesiyor. Noise Schedule, farkl\u0131 zaman ad\u0131mlar\u0131nda ne kadar g\u00fcr\u00fclt\u00fc eklenece\u011fini beliyor. Scheduler orneklerinden birisi 'DDPM' (\"Denoising Dif\u00fczyon Probabalistic Models\") makalesini buradan okuyabilirsiniz. Stable diffusion modeli icin bu noise ekleme sureci lineer olarak gerceklesmiyor, asagidaki gorselden gorebileceginiz gibi lineer bir gurultu ekleme sureci fotografin cok hizli bir sekilde pure noise formuna gecmesine sebebiyet veriyor bu yuzden, cosine kullaniyoruz. Temel olarak schedule iki gorevden sorumludur: Goruntuye timestampler ile birlikte iterative olarak gurultu eklemek Fotograftaki gurultu kaldirilirken bir sonraki timestampde goruntunun nasil guncellenecegini belirlemek Uygulama Scheduler'in nasil calistigina bakalim, egitim esnasinda 1000 step egittigimizi dusunelim. from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel import matplotlib.pyplot as plt scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000) # Setting the number of sampling steps: scheduler.set_timesteps(15) # See these in terms of the original 1000 steps used for training: print(scheduler.timesteps) # tensor([999.0000, 927.6429, 856.2857, 784.9286, 713.5714, 642.2143, 570.8571, # 499.5000, 428.1429, 356.7857, 285.4286, 214.0714, 142.7143, 71.3571, # 0.0000], dtype=torch.float64) # Look at the equivalent noise levels: print(scheduler.sigmas) # tensor([14.6146, 9.6826, 6.6780, 4.7746, 3.5221, 2.6666, 2.0606, 1.6156, # 1.2768, 1.0097, 0.7913, 0.6056, 0.4397, 0.2780, 0.0292, 0.0000]) Sigma parametresi goruntunun latent representasyonuna ne kadar gurultu eklendigini gosteriyor. # Plotting this noise schedule: plt.plot(scheduler.sigmas) plt.title('Noise Schedule') plt.xlabel('Sampling step') plt.ylabel('sigma') plt.show() Gurultu Eklemek Gorselmimize gurultu eklemenin nasil oldugunu gorsellestirelim. Autoencoder adiminda gorselimizin VAE'den gectikten sonraki encoded halini yani latent representationini cikartmistik. noise = torch.randn_like(encoded) # Random noise sampling_step = 10 # Equivalent to step 10 out of 15 in the schedule above # encoded_and_noised = scheduler.add_noise(encoded, noise, timestep) # Diffusers 0.3 and below encoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]])) latents_to_pil(encoded_and_noised.float())[0] # Display Sampling steps: [0, 10, 20, 25, 27, 29] Scheduler Tipleri Name Definition ddim Denoising Diffusion Implicit Models ddpm Denoising Diffusion Probabilistic Models singlestep_dpm_solver Singlestep DPM-Solver multistep_dpm_solver Multistep DPM-Solver heun Heun scheduler inspired by Karras et. al paper dpm_discrete DPM Discrete Scheduler inspired by Karras et. al paper dpm_discrete_ancestral DPM Discrete Scheduler with ancestral sampling inspired by Karras et. al paper stochastic_karras_ve Variance exploding, stochastic sampling from Karras et. al lms_discrete Linear multistep scheduler for discrete beta schedules pndm Pseudo numerical methods for diffusion models (PNDM) score_sde_ve variance exploding stochastic differential equation (VE-SDE) scheduler ipndm improved pseudo numerical methods for diffusion models (iPNDM) score_sde_vp Variance preserving stochastic differential equation (VP-SDE) scheduler euler Euler scheduler euler_ancestral Euler Ancestral scheduler vq_diffusion VQDiffusionScheduler repaint RePaint scheduler Lessons Learned Schedulers There are ancestral samplers (marked by the letter \"a\") whose output will keep changing as the number of steps increases, and the others, which will eventually settle on a final image. This final image is different between Karras and non-Karras samplers, but very similar within those groups. Then there's DPM fast, which doesn't feel particularly fast, and which always seems to produce inferior images for me. DPM adaptive is also its own thing, as it ignores step count and works with cfg scale instead. More cfg = more steps. I kind of like it when I'm not sure how many steps I should use, but the final step count is generally high. It can also take a long, long time if you use the \"AND\" prompt syntax - I have interrupted it after waiting for over 2000 steps. Most differences between the different samplers appear at low step counts < 20. Some produce distinguishable images faster and some slower, and may look very different in the early stages. That's random though, there's no good way to predict what those early images will turn into with more steps. In practice, the choice of samplers is just preference, there's actually very little difference in the long run. First, you have to understand what samplers are. These are discretized differential equations. I'm not going to go into these at all in this post, but I've covered them before. DDIM and PLMS were the original samplers. They were part of Latent Diffusion's repository. They stand for the papers that introduced them, Denoising Diffusion Implicit Models and Pseudo Numerical Methods for Diffusion Models on Manifolds. Almost all other samplers come from work done by @RiversHaveWings or Katherine Crowson, which is mostly contained in her work at this repository. She is listed as the principal researcher at Stability AI. Her notes for those samplers are as follows: \u2060Euler - Implements Algorithm 2 (Euler steps) from Karras et al. (2022) \u2060Euler_a - Ancestral sampling with Euler method steps. \u2060LMS - No information, but can be inferred that the name comes from linear multistep coefficients \u2060Heun - Implements Algorithm 2 (Heun steps) from Karras et al. (2022). \u2060DPM2 - A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022). \u2060DPM2 a - Ancestral sampling with DPM-Solver second-order steps \u2060DPM++ 2s a - Ancestral sampling with DPM-Solver++(2S) second-order steps \u2060DPM++ 2M - DPM-Solver++(2M) \u2060DPM++ SDE - DPM-Solver++ (stochastic) \u2060DPM fast - DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927 \u2060DPM adaptive - DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927 The 'Karras' versions of these weren't made by Karras as far as I can tell, but instead are using a variance-exploding scheduler from the Karras paper, which of course is extra confusing given that most of the other samplers were inspired by that paper in the first place. In terms of \"what will I get at high step counts\", most of the time you will get similar pictures from: \u2060Group A: Euler_a, DPM2 a, DPM++ 2S a, DPM fast (after many steps), DPM adaptive, DPM2 a Karras \u2060Group B: Euler, LMS, Heun, DPM2, DPM++ 2M, DDIM, PLMS \u2060Group C: LMS Karras, DPM2 Karras, DPM++ 2M Karras As far as convergence behavior: \u2060Does not converge: Euler_a, DPM2 a, DPM Fast, DDIM, PLMS, DPM adaptive, DPM2 a Karras \u2060Converges: Euler, LMS, Heun, DPM2, DPM++ 2M, LMS Karras, DPM2 Karras, DPM++ 2M Karras By required steps: \u2060Euler_a = Euler = DPM++2M = LMS Karras (image degraded at high steps) > \u2060LMS = DPM++ 2M Karras = Heun (slower) = DPM++ 2S a (slower) = DPM++ 2S a Karras > \u2060DDIM = PLMS = DPM2 (slower) = DPM 2 Karras> \u2060DPM Fast = DPM2 a (slower) These all give somewhat different results so a person could prefer the output of any of the models at a given CFG or step range. I do think that there is an argument to be made that DPM++ 2M and Euler_a are good generic samplers for most people, however, as they both resolve to a good picture at low seeds (sub-20) without a hit to iteration speed. DPM++ 2M has the advantage of converging to a single image more often (if you choose to run the same image at higher seed), but is slightly more prone to deformations at high CFG. To combine all the above: \u2060Fast, new, converges: DPM++ 2M, DPM++ 2M Karras \u2060Fast, doesn't converge: Euler_a, DPM2 a Karras \u2060Others worth considering: DPM2 a, LMS, DPM++ 2S a Karras \u2060Bugged: LMS Karras (at high steps \u2060Older, fast but maybe lower quality final result: Euler, LMS, Heun \u2060Slow: DDIM, PLMS, DPM2, DPM 2 Karras, DPM Fast, DPM2 a TL;DR These are confusingly named and mostly come from academic papers. The actual mechanisms of each sampler aren't really relevant to their outputs. In general PLMS, DDIM, or DPM fast are slower and give worse results. Instead, try out DPM++ 2M and Euler_a, along with DPM++ 2M Karras. These should all give good results at a low seed value.","title":"Scheduler"},{"location":"scheduler/#scheduler","text":"Scheduler, sampler ayni anlamlarda kullaniliyor. Diffusion prosesinde ana amacimiz fotografa yavas yavas gurultu eklemek ve her adimda modelin bu gurultuyu nasil kaldiracagini ogrenmesini beklemek. Modelin inference asamasinda verdigimiz random noise inputu kaldirarak goruntu olusturmasini istiyoruz. Goruntulere gurultu ekleme sureci lineer olmayan bir sekilde gerceklesiyor. Noise Schedule, farkl\u0131 zaman ad\u0131mlar\u0131nda ne kadar g\u00fcr\u00fclt\u00fc eklenece\u011fini beliyor. Scheduler orneklerinden birisi 'DDPM' (\"Denoising Dif\u00fczyon Probabalistic Models\") makalesini buradan okuyabilirsiniz. Stable diffusion modeli icin bu noise ekleme sureci lineer olarak gerceklesmiyor, asagidaki gorselden gorebileceginiz gibi lineer bir gurultu ekleme sureci fotografin cok hizli bir sekilde pure noise formuna gecmesine sebebiyet veriyor bu yuzden, cosine kullaniyoruz. Temel olarak schedule iki gorevden sorumludur: Goruntuye timestampler ile birlikte iterative olarak gurultu eklemek Fotograftaki gurultu kaldirilirken bir sonraki timestampde goruntunun nasil guncellenecegini belirlemek","title":"Scheduler"},{"location":"scheduler/#uygulama","text":"Scheduler'in nasil calistigina bakalim, egitim esnasinda 1000 step egittigimizi dusunelim. from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel import matplotlib.pyplot as plt scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000) # Setting the number of sampling steps: scheduler.set_timesteps(15) # See these in terms of the original 1000 steps used for training: print(scheduler.timesteps) # tensor([999.0000, 927.6429, 856.2857, 784.9286, 713.5714, 642.2143, 570.8571, # 499.5000, 428.1429, 356.7857, 285.4286, 214.0714, 142.7143, 71.3571, # 0.0000], dtype=torch.float64) # Look at the equivalent noise levels: print(scheduler.sigmas) # tensor([14.6146, 9.6826, 6.6780, 4.7746, 3.5221, 2.6666, 2.0606, 1.6156, # 1.2768, 1.0097, 0.7913, 0.6056, 0.4397, 0.2780, 0.0292, 0.0000]) Sigma parametresi goruntunun latent representasyonuna ne kadar gurultu eklendigini gosteriyor. # Plotting this noise schedule: plt.plot(scheduler.sigmas) plt.title('Noise Schedule') plt.xlabel('Sampling step') plt.ylabel('sigma') plt.show()","title":"Uygulama"},{"location":"scheduler/#gurultu-eklemek","text":"Gorselmimize gurultu eklemenin nasil oldugunu gorsellestirelim. Autoencoder adiminda gorselimizin VAE'den gectikten sonraki encoded halini yani latent representationini cikartmistik. noise = torch.randn_like(encoded) # Random noise sampling_step = 10 # Equivalent to step 10 out of 15 in the schedule above # encoded_and_noised = scheduler.add_noise(encoded, noise, timestep) # Diffusers 0.3 and below encoded_and_noised = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]])) latents_to_pil(encoded_and_noised.float())[0] # Display","title":"Gurultu Eklemek"},{"location":"scheduler/#sampling-steps-0-10-20-25-27-29","text":"","title":"Sampling steps: [0, 10, 20, 25, 27, 29]"},{"location":"scheduler/#scheduler-tipleri","text":"Name Definition ddim Denoising Diffusion Implicit Models ddpm Denoising Diffusion Probabilistic Models singlestep_dpm_solver Singlestep DPM-Solver multistep_dpm_solver Multistep DPM-Solver heun Heun scheduler inspired by Karras et. al paper dpm_discrete DPM Discrete Scheduler inspired by Karras et. al paper dpm_discrete_ancestral DPM Discrete Scheduler with ancestral sampling inspired by Karras et. al paper stochastic_karras_ve Variance exploding, stochastic sampling from Karras et. al lms_discrete Linear multistep scheduler for discrete beta schedules pndm Pseudo numerical methods for diffusion models (PNDM) score_sde_ve variance exploding stochastic differential equation (VE-SDE) scheduler ipndm improved pseudo numerical methods for diffusion models (iPNDM) score_sde_vp Variance preserving stochastic differential equation (VP-SDE) scheduler euler Euler scheduler euler_ancestral Euler Ancestral scheduler vq_diffusion VQDiffusionScheduler repaint RePaint scheduler","title":"Scheduler Tipleri"},{"location":"scheduler/#lessons-learned-schedulers","text":"There are ancestral samplers (marked by the letter \"a\") whose output will keep changing as the number of steps increases, and the others, which will eventually settle on a final image. This final image is different between Karras and non-Karras samplers, but very similar within those groups. Then there's DPM fast, which doesn't feel particularly fast, and which always seems to produce inferior images for me. DPM adaptive is also its own thing, as it ignores step count and works with cfg scale instead. More cfg = more steps. I kind of like it when I'm not sure how many steps I should use, but the final step count is generally high. It can also take a long, long time if you use the \"AND\" prompt syntax - I have interrupted it after waiting for over 2000 steps. Most differences between the different samplers appear at low step counts < 20. Some produce distinguishable images faster and some slower, and may look very different in the early stages. That's random though, there's no good way to predict what those early images will turn into with more steps. In practice, the choice of samplers is just preference, there's actually very little difference in the long run. First, you have to understand what samplers are. These are discretized differential equations. I'm not going to go into these at all in this post, but I've covered them before. DDIM and PLMS were the original samplers. They were part of Latent Diffusion's repository. They stand for the papers that introduced them, Denoising Diffusion Implicit Models and Pseudo Numerical Methods for Diffusion Models on Manifolds. Almost all other samplers come from work done by @RiversHaveWings or Katherine Crowson, which is mostly contained in her work at this repository. She is listed as the principal researcher at Stability AI. Her notes for those samplers are as follows: \u2060Euler - Implements Algorithm 2 (Euler steps) from Karras et al. (2022) \u2060Euler_a - Ancestral sampling with Euler method steps. \u2060LMS - No information, but can be inferred that the name comes from linear multistep coefficients \u2060Heun - Implements Algorithm 2 (Heun steps) from Karras et al. (2022). \u2060DPM2 - A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022). \u2060DPM2 a - Ancestral sampling with DPM-Solver second-order steps \u2060DPM++ 2s a - Ancestral sampling with DPM-Solver++(2S) second-order steps \u2060DPM++ 2M - DPM-Solver++(2M) \u2060DPM++ SDE - DPM-Solver++ (stochastic) \u2060DPM fast - DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927 \u2060DPM adaptive - DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927 The 'Karras' versions of these weren't made by Karras as far as I can tell, but instead are using a variance-exploding scheduler from the Karras paper, which of course is extra confusing given that most of the other samplers were inspired by that paper in the first place. In terms of \"what will I get at high step counts\", most of the time you will get similar pictures from: \u2060Group A: Euler_a, DPM2 a, DPM++ 2S a, DPM fast (after many steps), DPM adaptive, DPM2 a Karras \u2060Group B: Euler, LMS, Heun, DPM2, DPM++ 2M, DDIM, PLMS \u2060Group C: LMS Karras, DPM2 Karras, DPM++ 2M Karras As far as convergence behavior: \u2060Does not converge: Euler_a, DPM2 a, DPM Fast, DDIM, PLMS, DPM adaptive, DPM2 a Karras \u2060Converges: Euler, LMS, Heun, DPM2, DPM++ 2M, LMS Karras, DPM2 Karras, DPM++ 2M Karras By required steps: \u2060Euler_a = Euler = DPM++2M = LMS Karras (image degraded at high steps) > \u2060LMS = DPM++ 2M Karras = Heun (slower) = DPM++ 2S a (slower) = DPM++ 2S a Karras > \u2060DDIM = PLMS = DPM2 (slower) = DPM 2 Karras> \u2060DPM Fast = DPM2 a (slower) These all give somewhat different results so a person could prefer the output of any of the models at a given CFG or step range. I do think that there is an argument to be made that DPM++ 2M and Euler_a are good generic samplers for most people, however, as they both resolve to a good picture at low seeds (sub-20) without a hit to iteration speed. DPM++ 2M has the advantage of converging to a single image more often (if you choose to run the same image at higher seed), but is slightly more prone to deformations at high CFG. To combine all the above: \u2060Fast, new, converges: DPM++ 2M, DPM++ 2M Karras \u2060Fast, doesn't converge: Euler_a, DPM2 a Karras \u2060Others worth considering: DPM2 a, LMS, DPM++ 2S a Karras \u2060Bugged: LMS Karras (at high steps \u2060Older, fast but maybe lower quality final result: Euler, LMS, Heun \u2060Slow: DDIM, PLMS, DPM2, DPM 2 Karras, DPM Fast, DPM2 a TL;DR These are confusingly named and mostly come from academic papers. The actual mechanisms of each sampler aren't really relevant to their outputs. In general PLMS, DDIM, or DPM fast are slower and give worse results. Instead, try out DPM++ 2M and Euler_a, along with DPM++ 2M Karras. These should all give good results at a low seed value.","title":"Lessons Learned Schedulers"},{"location":"sd/","text":"Diffusion Models Temel notlar: Yuksek cozunurluklu goruntu islemek oldukca masraflidir cunku 128px bir goruntu 64px bir goruntunun tam 4 katidir bu baglamda yapilacak islemler quadratictir. Diffsuion modellerinde VAE (variational auto encoderlar) kullanilir, bu goruntunun ortulu bir uzayda daha kucuk bir representasyonunu saglar. 512x512x3 olan bir gorsel 64x64x4 boyutuna indirgenir. VAE ile birlikte gorsellerdeki redundant kisimlari atilmis olur, yeteri kadar data verildigi muddet VAE gorselleri latent spaceden tekrardan eski cozunurluge olusturabilmeyi ogrenir. Text Conditioning, diffusion modeli egitilirken goruntuye ek olarak bu goruntunun olusmasinda etkili olacak sekilde text datasinin da (caption) bilgi olarak verilmesine denir. Burda amaca gurultulu bir gorsel verildiginde modelin caption\u2019a uygun bir sekilde gurultuyu cozmesi ve goruntunun buna gore olusmasini amaclar. Inference aninda, baslangicta pure noise ve olusturmak istedigimiz goruntuye uygun olacak sekilde bir text veririz ve modelin random bir inputu text\u2019e gore olusturmasini isteriz. Text conditioning olusturmak icin yazilarin numerik bir representasyonunu olusturmamiz gerekiyor, bunu CLIP adi verilen OpenAI tarafindan gelistirilmis bir LLM modeli kullaniyoruz. CLIP, image captionlari uzerinde egitilmis, fotograflari ve yazilarini (fotograflarin captionlarini) karsislastirmamizi saglayan bir model. Stable diffusionda kullandigimiz promptlar once clip encoder'a iletilir ve SD 1.x serisi icin token basina 768 uzunlugunda bir embedding vektoru uretir. Bu SD 2.x versiyonlari icin 1024 uzunlugunda bir vektor. Sureci stabil tutmak adina tum promptlar 77 token ile sinirlandirilmistir. Yani daha uzun prompt girmeniz tokenlerin truncate edilmesine sebep olur. Son durumda CLIP'den elde ettigimiz matrix 77x768 uzunlugunda olacaktir. SD 2.x icin 77x1024. Cross Attention nedir? Her ne kadar text conditioning yapsak da, gun sonunda olusan gurultu baslangicta kullandigimiz noise inputa cok bagimli oldugunu goruyoruz. Bu mantikli cunku milyonlarca resimin caption'u cogunlukla resimin kendisinden bagimsiz captionlar iceriyor. Bu yuzden model descriptionlardan cok ogrenemiyor. Bu problemi ortadan kaldirmak icin CFG (Classifier Free Guidance) denilen bir yontem uyguluyoruz. Cok kisa bir tabirle model egitim sirasinda text bilgisi olmadan egitilir, inference aninda ise iki tahmin yapilir zero conditioning ve text conditioning, bu ikisi arasindaki farka bakarak bilr olcek olustururuz bu da CFG adi verilir. Super-resolution, Inpainting ve Depth to Image turunde 3 farkli conditioning de vardir, ayni textte oldugu gibi, super resolution fotografin yuksek cozunurluklu hali ve dusuk cozunurlukle hali uzerinde bir egitim gerceklestirilir, depth 2 image, midas modeli ile goruntunun kendisi ve derinlik haritasi cikartilmis haliyle conditionlanarak egitilir.","title":"Stable Diffusion"},{"location":"sd/#diffusion-models","text":"Temel notlar: Yuksek cozunurluklu goruntu islemek oldukca masraflidir cunku 128px bir goruntu 64px bir goruntunun tam 4 katidir bu baglamda yapilacak islemler quadratictir. Diffsuion modellerinde VAE (variational auto encoderlar) kullanilir, bu goruntunun ortulu bir uzayda daha kucuk bir representasyonunu saglar. 512x512x3 olan bir gorsel 64x64x4 boyutuna indirgenir. VAE ile birlikte gorsellerdeki redundant kisimlari atilmis olur, yeteri kadar data verildigi muddet VAE gorselleri latent spaceden tekrardan eski cozunurluge olusturabilmeyi ogrenir. Text Conditioning, diffusion modeli egitilirken goruntuye ek olarak bu goruntunun olusmasinda etkili olacak sekilde text datasinin da (caption) bilgi olarak verilmesine denir. Burda amaca gurultulu bir gorsel verildiginde modelin caption\u2019a uygun bir sekilde gurultuyu cozmesi ve goruntunun buna gore olusmasini amaclar. Inference aninda, baslangicta pure noise ve olusturmak istedigimiz goruntuye uygun olacak sekilde bir text veririz ve modelin random bir inputu text\u2019e gore olusturmasini isteriz. Text conditioning olusturmak icin yazilarin numerik bir representasyonunu olusturmamiz gerekiyor, bunu CLIP adi verilen OpenAI tarafindan gelistirilmis bir LLM modeli kullaniyoruz. CLIP, image captionlari uzerinde egitilmis, fotograflari ve yazilarini (fotograflarin captionlarini) karsislastirmamizi saglayan bir model. Stable diffusionda kullandigimiz promptlar once clip encoder'a iletilir ve SD 1.x serisi icin token basina 768 uzunlugunda bir embedding vektoru uretir. Bu SD 2.x versiyonlari icin 1024 uzunlugunda bir vektor. Sureci stabil tutmak adina tum promptlar 77 token ile sinirlandirilmistir. Yani daha uzun prompt girmeniz tokenlerin truncate edilmesine sebep olur. Son durumda CLIP'den elde ettigimiz matrix 77x768 uzunlugunda olacaktir. SD 2.x icin 77x1024. Cross Attention nedir? Her ne kadar text conditioning yapsak da, gun sonunda olusan gurultu baslangicta kullandigimiz noise inputa cok bagimli oldugunu goruyoruz. Bu mantikli cunku milyonlarca resimin caption'u cogunlukla resimin kendisinden bagimsiz captionlar iceriyor. Bu yuzden model descriptionlardan cok ogrenemiyor. Bu problemi ortadan kaldirmak icin CFG (Classifier Free Guidance) denilen bir yontem uyguluyoruz. Cok kisa bir tabirle model egitim sirasinda text bilgisi olmadan egitilir, inference aninda ise iki tahmin yapilir zero conditioning ve text conditioning, bu ikisi arasindaki farka bakarak bilr olcek olustururuz bu da CFG adi verilir. Super-resolution, Inpainting ve Depth to Image turunde 3 farkli conditioning de vardir, ayni textte oldugu gibi, super resolution fotografin yuksek cozunurluklu hali ve dusuk cozunurlukle hali uzerinde bir egitim gerceklestirilir, depth 2 image, midas modeli ile goruntunun kendisi ve derinlik haritasi cikartilmis haliyle conditionlanarak egitilir.","title":"Diffusion Models"},{"location":"text-embedding/","text":"Text Embedding Onceki bolumlerde inceledigimiz gibi diffusion modelimizi egitirken prompt ile istedigimzi ciktiyi uretmesi icin bir yazi ile gorsellerin latent representasyonlarini kondisyonluyorduk (conditioning) yani goruntuyu VAE'den gecirdikten sonra gorselin caption'ini da text embeddingden gecirip bu ikisini birlestiriyorduk. Tabi bu captionlar yazi formatinda veriler oldugu icin bunlari temsil edecek sayilar formlara da ihtiyacimiz var bu isleme text embedding deniyor. Goruntuler ve onlarin captionlari ile calistigimiz icin CLIP text encoder'i kullanmak mantikli olacaktir. Cok kisaca CLIP encoder, daha onceki bolumlerden hatirlayacaginiz gibi goruntuler ve ve goruntulerin captionlari ile OpenAI tarafindan egitilmis open source bir modeldi. CLIP Clip nedir? Uygulama text_encoder.text_model.embeddings # CLIPTextEmbeddings( # (token_embedding): Embedding(49408, 768) # (position_embedding): Embedding(77, 768) # ) # Our text prompt prompt = 'A picture of a puppy' Tokenizer # Turn the text into a sequnce of tokens: text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") text_input['input_ids'][0] # View the tokens # tensor([49406, 320, 1674, 539, 320, 6829, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407]) # See the individual tokens for t in text_input['input_ids'][0][:8]: # We'll just look at the first 7 to save you from a wall of '<|endoftext|>' print(t, tokenizer.decoder.get(int(t))) # Grab the output embeddings output_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0] print('Shape:', output_embeddings.shape) output_embeddings # Shape: torch.Size([1, 77, 768]) # tensor([[[-0.3884, 0.0229, -0.0522, ..., -0.4899, -0.3066, 0.0675], # [ 0.0290, -1.3258, 0.3085, ..., -0.5257, 0.9768, 0.6652], # [ 0.6942, 0.3538, 1.0991, ..., -1.5716, -1.2643, -0.0121], # ..., # [-0.0221, -0.0053, -0.0089, ..., -0.7303, -1.3830, -0.3011], # [-0.0062, -0.0246, 0.0065, ..., -0.7326, -1.3745, -0.2953], # [-0.0536, 0.0269, 0.0444, ..., -0.7159, -1.3634, -0.3075]]], # device='cuda:0', grad_fn=<NativeLayerNormBackward0>)","title":"Text Embedding"},{"location":"text-embedding/#text-embedding","text":"Onceki bolumlerde inceledigimiz gibi diffusion modelimizi egitirken prompt ile istedigimzi ciktiyi uretmesi icin bir yazi ile gorsellerin latent representasyonlarini kondisyonluyorduk (conditioning) yani goruntuyu VAE'den gecirdikten sonra gorselin caption'ini da text embeddingden gecirip bu ikisini birlestiriyorduk. Tabi bu captionlar yazi formatinda veriler oldugu icin bunlari temsil edecek sayilar formlara da ihtiyacimiz var bu isleme text embedding deniyor. Goruntuler ve onlarin captionlari ile calistigimiz icin CLIP text encoder'i kullanmak mantikli olacaktir. Cok kisaca CLIP encoder, daha onceki bolumlerden hatirlayacaginiz gibi goruntuler ve ve goruntulerin captionlari ile OpenAI tarafindan egitilmis open source bir modeldi.","title":"Text Embedding"},{"location":"text-embedding/#clip","text":"Clip nedir?","title":"CLIP"},{"location":"text-embedding/#uygulama","text":"text_encoder.text_model.embeddings # CLIPTextEmbeddings( # (token_embedding): Embedding(49408, 768) # (position_embedding): Embedding(77, 768) # ) # Our text prompt prompt = 'A picture of a puppy' Tokenizer # Turn the text into a sequnce of tokens: text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") text_input['input_ids'][0] # View the tokens # tensor([49406, 320, 1674, 539, 320, 6829, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, # 49407, 49407, 49407, 49407, 49407, 49407, 49407]) # See the individual tokens for t in text_input['input_ids'][0][:8]: # We'll just look at the first 7 to save you from a wall of '<|endoftext|>' print(t, tokenizer.decoder.get(int(t))) # Grab the output embeddings output_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0] print('Shape:', output_embeddings.shape) output_embeddings # Shape: torch.Size([1, 77, 768]) # tensor([[[-0.3884, 0.0229, -0.0522, ..., -0.4899, -0.3066, 0.0675], # [ 0.0290, -1.3258, 0.3085, ..., -0.5257, 0.9768, 0.6652], # [ 0.6942, 0.3538, 1.0991, ..., -1.5716, -1.2643, -0.0121], # ..., # [-0.0221, -0.0053, -0.0089, ..., -0.7303, -1.3830, -0.3011], # [-0.0062, -0.0246, 0.0065, ..., -0.7326, -1.3745, -0.2953], # [-0.0536, 0.0269, 0.0444, ..., -0.7159, -1.3634, -0.3075]]], # device='cuda:0', grad_fn=<NativeLayerNormBackward0>)","title":"Uygulama"},{"location":"vae/","text":"Autoencoder in Stable Diffusion def pil_to_latent(input_im): # Single image -> single latent in a batch (so size 1, 4, 64, 64) with torch.no_grad(): latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling return 0.18215 * latent.latent_dist.sample() def latents_to_pil(latents): # bath of latents -> list of images latents = (1 / 0.18215) * latents with torch.no_grad(): image = vae.decode(latents).sample image = (image / 2 + 0.5).clamp(0, 1) image = image.detach().cpu().permute(0, 2, 3, 1).numpy() images = (image * 255).round().astype(\"uint8\") pil_images = [Image.fromarray(image) for image in images] return pil_images # Load the image with PIL input_image = Image.open('cobanov.jpg').resize((512, 512)) input_image # Encode to the latent space encoded = pil_to_latent(input_image) encoded.shape # Let's visualize the four channels of this latent representation: fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for c in range(4): axs[c].imshow(encoded[0][c].cpu(), cmap='viridis') # Decode this latent representation back into an image decoded = latents_to_pil(encoded)[0] decoded Gozlerde ve burun altinda ufak bozulmalar mevcut fakat genel olarak VAE oldukca iyi bir performans gostererek fotografi tekrardan olusturmayi basardi. Sonuclar oldukca iyi 3x512x512 boyutunda bir fotografi 4x64x64 boyutuna indirgedik ve neredeyse kayipsiz bir sekilde geri getirmeyi basardik. Bu 48 katlik sikistirma anlamina geliyor. Orijinal Gorsel: 3x512x512 = 786432 Latent Representasyonu: 4 x 64 x 64 = 16384 Scale Factor: 786432 / 16384 = 48 Daha yuksek sikistirma oranlarina sahip AE modelleri de gorebilirsiniz, VQGAN'daki fp16 gibi fakat gunun sonunda fotografta daha fazla artifakt goreceksiniz. Peki neden AE kullaniyoruz biraz bunu konusalim. Eger fotograflarini oldugu gibi diffusion modelinde egitime sokarsaniz, bu cok fazla islem yapmaniza sebebiyet verecek, 786.432 pikselde islem yapmak yerine 16.384 piksel islem yapmak oldukca verimli, goruldugu gibi fotograf uzerinde de kaybimiz neredeyse minimum seviyede. Bazi modeller diffusion prosesini 64x64 boyutunda yapip daha sonrasinda goruntuyu upscale etmeye calisiyor, bu da cozumlerden biri, fakat bu cok kucuk bir gorseli defalarca upscale etmek anlamina geliyor, bunun icin ayri bir upscaling modeli de egitmeniz gerekiyor. Bunun yerine latent diffusion bu islemi gercek goruntuler yerine VAE'den altigi latent space'de yapiyor. Bu latent space bilgi anlaminda oldukca zengin ve fotografi tekrardan olusturmak icin yeterli bilgiyi icerdigi icin diffusion prosesine iyi bir optimizasyon sagliyor.","title":"VAE"},{"location":"vae/#autoencoder-in-stable-diffusion","text":"def pil_to_latent(input_im): # Single image -> single latent in a batch (so size 1, 4, 64, 64) with torch.no_grad(): latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling return 0.18215 * latent.latent_dist.sample() def latents_to_pil(latents): # bath of latents -> list of images latents = (1 / 0.18215) * latents with torch.no_grad(): image = vae.decode(latents).sample image = (image / 2 + 0.5).clamp(0, 1) image = image.detach().cpu().permute(0, 2, 3, 1).numpy() images = (image * 255).round().astype(\"uint8\") pil_images = [Image.fromarray(image) for image in images] return pil_images # Load the image with PIL input_image = Image.open('cobanov.jpg').resize((512, 512)) input_image # Encode to the latent space encoded = pil_to_latent(input_image) encoded.shape # Let's visualize the four channels of this latent representation: fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for c in range(4): axs[c].imshow(encoded[0][c].cpu(), cmap='viridis') # Decode this latent representation back into an image decoded = latents_to_pil(encoded)[0] decoded Gozlerde ve burun altinda ufak bozulmalar mevcut fakat genel olarak VAE oldukca iyi bir performans gostererek fotografi tekrardan olusturmayi basardi. Sonuclar oldukca iyi 3x512x512 boyutunda bir fotografi 4x64x64 boyutuna indirgedik ve neredeyse kayipsiz bir sekilde geri getirmeyi basardik. Bu 48 katlik sikistirma anlamina geliyor. Orijinal Gorsel: 3x512x512 = 786432 Latent Representasyonu: 4 x 64 x 64 = 16384 Scale Factor: 786432 / 16384 = 48 Daha yuksek sikistirma oranlarina sahip AE modelleri de gorebilirsiniz, VQGAN'daki fp16 gibi fakat gunun sonunda fotografta daha fazla artifakt goreceksiniz. Peki neden AE kullaniyoruz biraz bunu konusalim. Eger fotograflarini oldugu gibi diffusion modelinde egitime sokarsaniz, bu cok fazla islem yapmaniza sebebiyet verecek, 786.432 pikselde islem yapmak yerine 16.384 piksel islem yapmak oldukca verimli, goruldugu gibi fotograf uzerinde de kaybimiz neredeyse minimum seviyede. Bazi modeller diffusion prosesini 64x64 boyutunda yapip daha sonrasinda goruntuyu upscale etmeye calisiyor, bu da cozumlerden biri, fakat bu cok kucuk bir gorseli defalarca upscale etmek anlamina geliyor, bunun icin ayri bir upscaling modeli de egitmeniz gerekiyor. Bunun yerine latent diffusion bu islemi gercek goruntuler yerine VAE'den altigi latent space'de yapiyor. Bu latent space bilgi anlaminda oldukca zengin ve fotografi tekrardan olusturmak icin yeterli bilgiyi icerdigi icin diffusion prosesine iyi bir optimizasyon sagliyor.","title":"Autoencoder in Stable Diffusion"},{"location":"vit/","text":"Vision Transformers ViTs Introduction 2022 yili yapay zekanin yili oldu, yapay zeka sanatinda, dogal dil islemede, goruntu islemede, ses teknolojilerinde inanilmaz gelismeler yasandi. Hem Huggingface hem OpenAI firtinalar kopardi. Onceki yillara nazaran bu yapay zeka teknolojileri hem demokratiklesti hem de son kullaniciya daha fazla ulasma firsati buldu. Bugun sizinle bu gelismelerden biri olan Vision Transformerlardan bahsedecegim. Makale giderek derinlesip tekniklesiyor, bu yuzden tum akisi basitten karmasiga olacak sekilde siraladim. Bu konu ne kadar ilginizi cekiyorsa o kadar ileri gidebilirsiniz. Bu makaleyi yazarken bircok kaynaktan, bloglardan, kendi bilgilerimden hatta ChatGPT'den dahi yararlandim. Daha derin okumalar yapmak isterseniz lutfen kaynaklar bolumune goz atin! TL; DR 2022'de Vision Transformer (ViT), \u015fu anda bilgisayar g\u00f6r\u00fc\u015f\u00fcnde son teknoloji olan ve bu nedenle farkl\u0131 g\u00f6r\u00fcnt\u00fc tan\u0131ma g\u00f6revlerinde yayg\u0131n olarak kullan\u0131lan evri\u015fimli sinir a\u011flar\u0131na (CNN'ler) rekabet\u00e7i bir alternatif olarak ortaya \u00e7\u0131kt\u0131. ViT modelleri, hesaplama verimlili\u011fi ve do\u011frulu\u011fu a\u00e7\u0131s\u0131ndan mevcut en son teknolojiye (CNN) neredeyse 4 kat daha iyi performans g\u00f6steriyor. Bu makale a\u015fa\u011f\u0131daki konulardan bahsedecegim: Vision Transformer (ViT) nedir? Vision Transformers vs Convolutional Neural Networks Attention Mekanizmasi ViT Implementasyonlari ViT Mimarisi Vision Transformers'\u0131n Kullan\u0131m ve Uygulamasi Vision Transformers Uzun yillardir CNN algoritmalari goruntu isleme konularinda neredeyse tek cozumumuzdu. ResNet , EfficientNet , Inception vb. gibi tum mimariler temelde CNN mimarilerini kullanarak goruntu isleme problemlerimizi cozmede bize yardimci oluyordu. Bugun sizinle goruntu isleme konusunda farkli bir yaklasim olan ViT'ler yani Vision Transformerlari inceleyecegiz. Aslinda Transformer kavrami NLP alaninda yurutulen teknolojiler icin ortaya kondu. Attention Is All You Need adiyla yayinlanan makale NLP problemlerinin cozumu icin devrimsel cozumler getirdi, artik Transformer-based mimarilar NLP gorevleri icin standart bir hale geldi. Cok da uzun bir sure gecmeden dogal dil alaninda kullanilan bu mimari goruntu alaninda da ufak degisikliklerle uyarlandi. Bu calismayi An image is worth 16x16 words olarak bu linkteki paperdan okuyabilirsiniz. Asagida daha detayli anlatacagim fakat surec temel olarak bir goruntuyu 16x16 boyutlu parcalara ayirarak embeddinglerini cikartmak uzerine calisiyor. Temel bazi konulari anlatmadan bu mekanikleri aciklamak cok zor bu yuzden hiz kaybetmeden konuyu daha iyi anlamak icin alt basliklara gecelim. ViTs vs. CNN Bu iki mimariyi karsilastirdigimizda acik ara ViTlerin cok daha etkileyici oldugunu gorebiliyoruz. Vision Transformerlar, training i\u00e7in daha az hesaplama kayna\u011f\u0131 kullanirken ayni zamanda, evri\u015fimli sinir a\u011flar\u0131na (CNN) daha iyi performans gosteriyor. Birazdan asagida daha detayli olarak anlatacagim fakat temelde CNN'ler piksel dizilerini kullan\u0131r, ViT ise g\u00f6r\u00fcnt\u00fcleri sabit boyutlu ufak parcalara boler. Her bir parca transformer encoder ile patch, positional vs. embeddingleri cikartilir (asagida anlatacagim konular bunlari iceriyor). Ayr\u0131ca ViT modelleri, hesaplama verimlili\u011fi ve do\u011frulu\u011fu s\u00f6z konusu oldu\u011funda CNN'lerden neredeyse d\u00f6rt kat daha iyi performans g\u00f6steriyorlar. ViT'deki self-attention katman\u0131, bilgilerin genel olarak g\u00f6r\u00fcnt\u00fcn\u00fcn tamam\u0131na yerle\u015ftirilmesini m\u00fcmk\u00fcn kiliyor bu demek oluyor ki yeniden birlestirmek istedigimizde veya yenilerini olusturmak istedigimizde bu bilgi de elimizde olacak, yani modele bunlari da ogretiyoruz. Raw gorselleri (solda) ViT-S/16 modeliyle attention haritalari (sagda) Kaynak: When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations https://arxiv.org/abs/2106.01548 Attention Mekanizmasi Bu bolum ChatGPT ile yazildi \u00d6zetle, NLP (Do\u011fal Dil \u0130\u015fleme) i\u00e7in geli\u015ftirilen attention(dikkat) mekanizmalar\u0131, yapay sinir a\u011f\u0131 modellerinin girdiyi daha iyi i\u015fleyip anlamas\u0131na yard\u0131mc\u0131 olmak i\u00e7in kullan\u0131l\u0131r. Bu mekanizmalar, girdinin farkl\u0131 b\u00f6l\u00fcmlerine farkl\u0131 a\u011f\u0131rl\u0131k verilerek \u00e7al\u0131\u015f\u0131r, bu sayede model girdiyi i\u015flerken belli b\u00f6l\u00fcmlere daha fazla dikkat eder. Attention mekanizmalar\u0131, dot-product attention, multi-head attention ve transformer attention gibi farkl\u0131 t\u00fcrleri geli\u015ftirilmi\u015ftir. Bu mekanizmalar her birisi biraz farkl\u0131 \u015fekilde \u00e7al\u0131\u015fsa da, hepsi girdinin farkl\u0131 b\u00f6l\u00fcmlerine farkl\u0131 a\u011f\u0131rl\u0131k verilerek modelin belli b\u00f6l\u00fcmlere daha fazla dikkat etmesine izin verme ilkesi \u00fczerine \u00e7al\u0131\u015f\u0131r. \u00d6rne\u011fin, bir makine \u00e7eviri g\u00f6revinde, bir attention mekanizmas\u0131 modelin kaynak dil c\u00fcmlesindeki belli kelimeleri \u00fcreterek hedef dil c\u00fcmlesine dikkat etmesine izin verebilir. Bu, modelin daha do\u011fru \u00e7eviriler \u00fcretebilmesine yard\u0131mc\u0131 olur, \u00e7\u00fcnk\u00fc kaynak dil kelimelerinin anlam ve ba\u011flam\u0131n\u0131 dikkate alarak \u00e7eviri \u00fcretebilir. Genel olarak, attention mekanizmalar\u0131 bir\u00e7ok state-of-the-art NLP modelinin bir par\u00e7as\u0131d\u0131r ve bu modellerin \u00e7e\u015fitli g\u00f6revlerde performans\u0131n\u0131 geli\u015ftirme konusunda \u00e7ok etkili oldu\u011fu g\u00f6sterilmi\u015ftir. ChatGPT sonu Aslinda konuya hakim biri icin daha iyi bir aciklama fakat cok kisa bu alanda hicbir bilgisi olmayan birisi icin basitlestirilmis bir aciklama burada harika bir sekilde anlatilmis Mechanics of Seq2seq Models With Attention Illustrated Transformer ViT Implementasyonlari Fine-tune edilmis ve pre-trained ViT modelleri Google Research 'un Github'\u0131nda mevcut: https://github.com/google-research/vision_transformer Pytorch Implementasyonlari lucidrains'in Github reposunda bulabilirsiniz: https://github.com/lucidrains/vit-pytorch Ayni zamanda timm kullanarak hazir modelleri hizlica kullanabilirsiniz. https://github.com/rwightman/pytorch-image-models Mimari ViT mimarisi birka\u00e7 a\u015famadan olu\u015fuyor: Patch + Position Embedding (inputs) - Giri\u015f g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc bir dizi g\u00f6r\u00fcnt\u00fc parcalarina (patches) d\u00f6n\u00fc\u015ft\u00fcr\u00fcr ve parcalarin hangi s\u0131rayla geldi\u011fini bilmek icin bir konum numaras\u0131 ekler. Linear projection of flattened patches (Embedded Patches) - G\u00f6r\u00fcnt\u00fc parcalari embeddinglere donusturulur, g\u00f6r\u00fcnt\u00fcleri direkt kullanmak yerine embeddingleri kullanman\u0131n yarar\u0131, embeddingler g\u00f6r\u00fcnt\u00fcn\u00fcn e\u011fitimle \u00f6\u011frenilebilir bir temsili olmas\u0131d\u0131r. Norm - Bir sinir a\u011f\u0131n\u0131 d\u00fczenli hale getirmek (overfitting'i azaltmak) i\u00e7in bir teknik olan \"Layer Normalization\" veya \"LayerNorm\"un k\u0131saltmas\u0131d\u0131r. Multi-Head Attention - Bu, Multi-Headed Self-Attention layer veya k\u0131saca \"MSA\" d\u0131r. MLP (Multilayer perceptron) - Genellikle herhangi bir feed-forward (ileri besleme) katman\u0131 koleksiyonu olarak dusunebilirsiniz. Transformer Encoder - Transformer Encoder, yukar\u0131da listelenen katmanlar\u0131n bir koleksiyonudur. Transformer Encoderin i\u00e7inde iki skip (atlama) ba\u011flant\u0131s\u0131 vard\u0131r (\"+\" sembolleri), katman\u0131n girdilerinin do\u011frudan sonraki katmanlar\u0131n yan\u0131 s\u0131ra hemen sonraki katmanlara beslendi\u011fi anlam\u0131na gelir. Genel ViT mimarisi, birbiri \u00fczerine y\u0131\u011f\u0131lm\u0131\u015f bir dizi Transformer kodlay\u0131c\u0131dan olu\u015fur. MLP Head - Bu, mimarinin \u00e7\u0131kt\u0131 katman\u0131d\u0131r, bir girdinin \u00f6\u011frenilen \u00f6zelliklerini bir s\u0131n\u0131f \u00e7\u0131kt\u0131s\u0131na d\u00f6n\u00fc\u015ft\u00fcr\u00fcr. G\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 \u00fczerinde \u00e7al\u0131\u015ft\u0131\u011f\u0131m\u0131z i\u00e7in buna \"s\u0131n\u0131fland\u0131r\u0131c\u0131 kafa\" da diyebilirsiniz. MLP head yap\u0131s\u0131 MLP blo\u011funa benzer. ViT Mimarisi Patch Embeddings Standart Transformer, giri\u015fi tek boyutlu token embedding dizisi olarak al\u0131r. 2B g\u00f6r\u00fcnt\u00fcleri i\u015flemek i\u00e7in x\u2208R^{H\u00d7W\u00d7C} g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc d\u00fczle\u015ftirilmi\u015f 2B patchlere (goruntu parcalarina) yeniden \u015fekillendiriyoruz. Burada, (H, W) orijinal g\u00f6r\u00fcnt\u00fcn\u00fcn \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr ve (P, P) her g\u00f6r\u00fcnt\u00fc par\u00e7as\u0131n\u0131n \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr. Resim sabit boyutlu parcalara b\u00f6l\u00fcnm\u00fc\u015ft\u00fcr, a\u015fa\u011f\u0131daki resimde patch (parca) boyutu 16\u00d716 olarak al\u0131nm\u0131\u015ft\u0131r. Yani g\u00f6r\u00fcnt\u00fcn\u00fcn boyutlar\u0131 48\u00d748 olacakt\u0131r. (Cunku 3 kanal var) Self-attention maliyeti quadratictir. G\u00f6r\u00fcnt\u00fcn\u00fcn her pikselini girdi olarak iletirsek, Self-attention her pikselin di\u011fer t\u00fcm piksellerle ilgilenmesini gerektirir. Self-attention ikinci dereceden maliyeti \u00e7ok maliyetli olacak ve ger\u00e7ek\u00e7i girdi boyutuna \u00f6l\u00e7eklenmeyecek; bu nedenle, g\u00f6r\u00fcnt\u00fc parcalara b\u00f6l\u00fcn\u00fcr. Yani sair burada her pikselle ugrasmak sonsuza kadar surecegi icin 16x16 boyutlu goruntu bolumlerinin embeddinglerini almanin parametre sayisini dusureceginden bahsediyor. import matplotlib.pyplot as plt from PIL import Image import numpy as np img = Image.open('cobanov-profile.jpg') img.thumbnail((224, 224)) array_img = np.array(img) array_img.shape # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\ \\nNumber of patches per column: {num_patches}\\ \\nTotal patches: {num_patches*num_patches}\\ \\nPatch size: {patch_size} pixels x {patch_size} pixels\") # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float ncols=img_size // patch_size, figsize=(num_patches, num_patches), sharex=True, sharey=True) # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels)) axs[i, j].imshow(array_img[patch_height:patch_height+patch_size, # iterate through height patch_width:patch_width+patch_size, # iterate through width :]) # get all color channels # Set up label information, remove the ticks for clarity and set labels to outside axs[i, j].set_ylabel(i+1, rotation=\"horizontal\", horizontalalignment=\"right\", verticalalignment=\"center\") axs[i, j].set_xlabel(j+1) axs[i, j].set_xticks([]) axs[i, j].set_yticks([]) axs[i, j].label_outer() # Set a super title plt.show() Linear Projection of Flattened Patches Parcalari Transformer blo\u011funa ge\u00e7irmeden \u00f6nce, makalenin yazarlar\u0131 yamalar\u0131 \u00f6nce do\u011frusal bir projeksiyondan ge\u00e7irmeyi faydal\u0131 bulmuslar. Bir yamay\u0131 al\u0131p b\u00fcy\u00fck bir vekt\u00f6re a\u00e7arlar ve patch embeddingler (goruntu parcalarinin gommeleri? veya embedddingleri) olu\u015fturmak i\u00e7in embedding matrisiyle \u00e7arparlar ve bu konumsal g\u00f6mmeyle (positional embeddings) birlikte transformat\u00f6re giden \u015feydir. Her goruntu parcasi (patches), t\u00fcm piksel kanallar\u0131n\u0131 bir yamada birle\u015ftirerek ve ard\u0131ndan bunu do\u011frusal olarak istenen giri\u015f boyutuna yans\u0131tarak embed edilen bir 1B patch'e d\u00fczle\u015ftirilir. Ne demek istedigimi bu gorselde cok daha iyi anlayacaginizi dusunuyorum. Kaynak: Face Transformer for Recognition Positional embeddings Nasil konusurken dilde kelimelerin s\u0131ras\u0131 kurdugunuz cumlenin anlamini tamamen degistiriyorsa, goruntuler uzerinde de buna dikkat etmek gerekiyor. Maalesef transformerlar, patch embeddinglerin \"s\u0131ras\u0131n\u0131\" dikkate alan herhangi bir varsay\u0131lan mekanizmaya sahip de\u011filler. Bir yapboz yaptiginizi dusunun, elinizdeki parcalar (yani onceki adimlarda yaptigimiz patch embeddingler) karisik bir duzende geldiginde goruntunun tamaminda ne oldugunu anlamak oldukca zordur, bu transformerlar i\u00e7in de ge\u00e7erli. Modelin yapboz par\u00e7alar\u0131n\u0131n s\u0131ras\u0131n\u0131 veya konumunu \u00e7\u0131karmas\u0131n\u0131 sa\u011flaman\u0131n bir yoluna ihtiyac\u0131m\u0131z var. Transformerlar, giri\u015f elemanlar\u0131n\u0131n yap\u0131s\u0131ndan ba\u011f\u0131ms\u0131zd\u0131r. Her yamaya \u00f6\u011frenilebilir positional embeddings (konum yerle\u015ftirmeleri) eklemek, modelin g\u00f6r\u00fcnt\u00fcn\u00fcn yap\u0131s\u0131 hakk\u0131nda bilgi edinmesine olanak tan\u0131r. Positional embeddingler de, bu d\u00fczeni modele aktarmamizi sagliyor. ViT i\u00e7in, bu positional embeddingler, patch embeddingler ile ayn\u0131 boyutlulu\u011fa sahip \u00f6\u011frenilmi\u015f vekt\u00f6rlerdir. Bu positional embeddingler, e\u011fitim s\u0131ras\u0131nda ve (bazen) ince ayar s\u0131ras\u0131nda \u00f6\u011frenilir. E\u011fitim s\u0131ras\u0131nda, bu embeddingler, \u00f6zellikle ayn\u0131 s\u00fctunu ve sat\u0131r\u0131 payla\u015fan kom\u015fu konum yerle\u015ftirmelerine y\u00fcksek benzerlik g\u00f6sterdikleri vekt\u00f6r uzaylar\u0131nda birle\u015fir. Transformer Encoding Multi-Head Self Attention Layer(MSP) birden fazla attention ciktisini lineer olarak beklenen boyutlara esitlemek i\u00e7in kullanilir. MSP, g\u00f6r\u00fcnt\u00fcdeki yerel ve global ba\u011f\u0131ml\u0131l\u0131klar\u0131 \u00f6\u011frenmeye yard\u0131mc\u0131 olur. Multi-Layer Perceptrons(MLP) - Klasik sinir agi katmani fakat aktivasyon fonksiyonu olarak GELU Gaussian Error Linear Units kullaniyoruz. Layer Norm(LN) e\u011fitim g\u00f6r\u00fcnt\u00fcleri aras\u0131nda herhangi bir yeni ba\u011f\u0131ml\u0131l\u0131k getirmedi\u011finden her bloktan \u00f6nce uygulan\u0131r. E\u011fitim s\u00fcresini ve genelleme performans\u0131n\u0131 iyile\u015ftirmeye yard\u0131mc\u0131 olur. Burada Misra'nin harika bir videosu var. Bu da Paper . Residual connections gradyanlar\u0131n do\u011frusal olmayan aktivasyonlardan ge\u00e7meden do\u011frudan a\u011f \u00fczerinden akmas\u0131na izin verdi\u011fi i\u00e7in her bloktan sonra uygulan\u0131r. Image classification i\u00e7in, \u00f6n e\u011fitim zaman\u0131nda bir hidden layer ve fine-tuning i\u00e7in tek bir linear layer ile MLP kullan\u0131larak bir classification head uygulan\u0131r. ViT'nin \u00fcst katmanlar\u0131 global \u00f6zellikleri \u00f6\u011frenirken, alt katmanlar hem global hem de yerel \u00f6zellikleri \u00f6\u011frenir. Bu da aslinda ViT'nin daha genel kal\u0131plar\u0131 \u00f6\u011frenmesini sagliyor. Toparlama Evet oldukca fazla terim, teori ve mimari inceledik kafalarda daha iyi oturtmak adina bu gif surecin nasil isledigini guzel bir sekilde ozetliyor. Usage Eger basitce bir ViT kullanmak isterseniz bunun icin ufacik bir rehberi de buraya ekliyorum. Muhtemelen denk gelmissinizdir artik yapay zeka alaninda yeni bir sey ciktiginda birakin bunun implementasyonunu kullanimini da birkac satir koda indirmek moda oldu. Yukarida anlattigim her seyi birkac satir Python koduyla nasil yapildigina bakalim. Colab linki: https://colab.research.google.com/drive/1sPafxIo6s1BBjHbl9e0b_DYGlb2AMBC3?usp=sharing Oncelikle pretrained model instantiate (orneklendirmek?) edelim. import timm model = timm.create_model('vit_base_patch16_224', pretrained=True) model.eval() Goruntumuzu yukleyip on islemelerini tamamlayim. Ben burada twitter profil fotograimi kullanacagim. # eger kendiniz bir gorsel vermek isterseniz # asagidaki kodda bu kismi comment'e alip, # filename kismina local path verebilirsiniz. url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\") urllib.request.urlretrieve(url, filename) import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config({}, model=model) transform = create_transform(**config) url, filename = (\"https://pbs.twimg.com/profile_images/1594739904642154497/-7kZ3Sf3_400x400.jpg\", \"mert.jpg\") urllib.request.urlretrieve(url, filename) img = Image.open(filename).convert('RGB') tensor = transform(img).unsqueeze(0) # transform and add batch dimension Tahminleri alalim import torch with torch.no_grad(): out = model(tensor) probabilities = torch.nn.functional.softmax(out[0], dim=0) print(probabilities.shape) # prints: torch.Size([1000]) En populer 5 tahminin siniflarina bakalim. # Get imagenet class mappings url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\") urllib.request.urlretrieve(url, filename) with open(\"imagenet_classes.txt\", \"r\") as f: categories = [s.strip() for s in f.readlines()] # Print top categories per image top5_prob, top5_catid = torch.topk(probabilities, 5) for i in range(top5_prob.size(0)): print(categories[top5_catid[i]], top5_prob[i].item()) trench coat 0.422695130109787 bulletproof vest 0.18995067477226257 suit 0.06873432546854019 sunglasses 0.02222270704805851 sunglass 0.020680639892816544 Sources https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview https://theaisummer.com/vision-transformer/ https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2 https://viso.ai/deep-learning/vision-transformer-vit/ https://arxiv.org/abs/2106.01548","title":"ViT Visual Transformers"},{"location":"vit/#vision-transformers-vits","text":"","title":"Vision Transformers ViTs"},{"location":"vit/#introduction","text":"2022 yili yapay zekanin yili oldu, yapay zeka sanatinda, dogal dil islemede, goruntu islemede, ses teknolojilerinde inanilmaz gelismeler yasandi. Hem Huggingface hem OpenAI firtinalar kopardi. Onceki yillara nazaran bu yapay zeka teknolojileri hem demokratiklesti hem de son kullaniciya daha fazla ulasma firsati buldu. Bugun sizinle bu gelismelerden biri olan Vision Transformerlardan bahsedecegim. Makale giderek derinlesip tekniklesiyor, bu yuzden tum akisi basitten karmasiga olacak sekilde siraladim. Bu konu ne kadar ilginizi cekiyorsa o kadar ileri gidebilirsiniz. Bu makaleyi yazarken bircok kaynaktan, bloglardan, kendi bilgilerimden hatta ChatGPT'den dahi yararlandim. Daha derin okumalar yapmak isterseniz lutfen kaynaklar bolumune goz atin!","title":"Introduction"},{"location":"vit/#tl-dr","text":"2022'de Vision Transformer (ViT), \u015fu anda bilgisayar g\u00f6r\u00fc\u015f\u00fcnde son teknoloji olan ve bu nedenle farkl\u0131 g\u00f6r\u00fcnt\u00fc tan\u0131ma g\u00f6revlerinde yayg\u0131n olarak kullan\u0131lan evri\u015fimli sinir a\u011flar\u0131na (CNN'ler) rekabet\u00e7i bir alternatif olarak ortaya \u00e7\u0131kt\u0131. ViT modelleri, hesaplama verimlili\u011fi ve do\u011frulu\u011fu a\u00e7\u0131s\u0131ndan mevcut en son teknolojiye (CNN) neredeyse 4 kat daha iyi performans g\u00f6steriyor. Bu makale a\u015fa\u011f\u0131daki konulardan bahsedecegim: Vision Transformer (ViT) nedir? Vision Transformers vs Convolutional Neural Networks Attention Mekanizmasi ViT Implementasyonlari ViT Mimarisi Vision Transformers'\u0131n Kullan\u0131m ve Uygulamasi","title":"TL; DR"},{"location":"vit/#vision-transformers","text":"Uzun yillardir CNN algoritmalari goruntu isleme konularinda neredeyse tek cozumumuzdu. ResNet , EfficientNet , Inception vb. gibi tum mimariler temelde CNN mimarilerini kullanarak goruntu isleme problemlerimizi cozmede bize yardimci oluyordu. Bugun sizinle goruntu isleme konusunda farkli bir yaklasim olan ViT'ler yani Vision Transformerlari inceleyecegiz. Aslinda Transformer kavrami NLP alaninda yurutulen teknolojiler icin ortaya kondu. Attention Is All You Need adiyla yayinlanan makale NLP problemlerinin cozumu icin devrimsel cozumler getirdi, artik Transformer-based mimarilar NLP gorevleri icin standart bir hale geldi. Cok da uzun bir sure gecmeden dogal dil alaninda kullanilan bu mimari goruntu alaninda da ufak degisikliklerle uyarlandi. Bu calismayi An image is worth 16x16 words olarak bu linkteki paperdan okuyabilirsiniz. Asagida daha detayli anlatacagim fakat surec temel olarak bir goruntuyu 16x16 boyutlu parcalara ayirarak embeddinglerini cikartmak uzerine calisiyor. Temel bazi konulari anlatmadan bu mekanikleri aciklamak cok zor bu yuzden hiz kaybetmeden konuyu daha iyi anlamak icin alt basliklara gecelim.","title":"Vision Transformers"},{"location":"vit/#vits-vs-cnn","text":"Bu iki mimariyi karsilastirdigimizda acik ara ViTlerin cok daha etkileyici oldugunu gorebiliyoruz. Vision Transformerlar, training i\u00e7in daha az hesaplama kayna\u011f\u0131 kullanirken ayni zamanda, evri\u015fimli sinir a\u011flar\u0131na (CNN) daha iyi performans gosteriyor. Birazdan asagida daha detayli olarak anlatacagim fakat temelde CNN'ler piksel dizilerini kullan\u0131r, ViT ise g\u00f6r\u00fcnt\u00fcleri sabit boyutlu ufak parcalara boler. Her bir parca transformer encoder ile patch, positional vs. embeddingleri cikartilir (asagida anlatacagim konular bunlari iceriyor). Ayr\u0131ca ViT modelleri, hesaplama verimlili\u011fi ve do\u011frulu\u011fu s\u00f6z konusu oldu\u011funda CNN'lerden neredeyse d\u00f6rt kat daha iyi performans g\u00f6steriyorlar. ViT'deki self-attention katman\u0131, bilgilerin genel olarak g\u00f6r\u00fcnt\u00fcn\u00fcn tamam\u0131na yerle\u015ftirilmesini m\u00fcmk\u00fcn kiliyor bu demek oluyor ki yeniden birlestirmek istedigimizde veya yenilerini olusturmak istedigimizde bu bilgi de elimizde olacak, yani modele bunlari da ogretiyoruz. Raw gorselleri (solda) ViT-S/16 modeliyle attention haritalari (sagda) Kaynak: When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations https://arxiv.org/abs/2106.01548","title":"ViTs vs. CNN"},{"location":"vit/#attention-mekanizmasi","text":"Bu bolum ChatGPT ile yazildi \u00d6zetle, NLP (Do\u011fal Dil \u0130\u015fleme) i\u00e7in geli\u015ftirilen attention(dikkat) mekanizmalar\u0131, yapay sinir a\u011f\u0131 modellerinin girdiyi daha iyi i\u015fleyip anlamas\u0131na yard\u0131mc\u0131 olmak i\u00e7in kullan\u0131l\u0131r. Bu mekanizmalar, girdinin farkl\u0131 b\u00f6l\u00fcmlerine farkl\u0131 a\u011f\u0131rl\u0131k verilerek \u00e7al\u0131\u015f\u0131r, bu sayede model girdiyi i\u015flerken belli b\u00f6l\u00fcmlere daha fazla dikkat eder. Attention mekanizmalar\u0131, dot-product attention, multi-head attention ve transformer attention gibi farkl\u0131 t\u00fcrleri geli\u015ftirilmi\u015ftir. Bu mekanizmalar her birisi biraz farkl\u0131 \u015fekilde \u00e7al\u0131\u015fsa da, hepsi girdinin farkl\u0131 b\u00f6l\u00fcmlerine farkl\u0131 a\u011f\u0131rl\u0131k verilerek modelin belli b\u00f6l\u00fcmlere daha fazla dikkat etmesine izin verme ilkesi \u00fczerine \u00e7al\u0131\u015f\u0131r. \u00d6rne\u011fin, bir makine \u00e7eviri g\u00f6revinde, bir attention mekanizmas\u0131 modelin kaynak dil c\u00fcmlesindeki belli kelimeleri \u00fcreterek hedef dil c\u00fcmlesine dikkat etmesine izin verebilir. Bu, modelin daha do\u011fru \u00e7eviriler \u00fcretebilmesine yard\u0131mc\u0131 olur, \u00e7\u00fcnk\u00fc kaynak dil kelimelerinin anlam ve ba\u011flam\u0131n\u0131 dikkate alarak \u00e7eviri \u00fcretebilir. Genel olarak, attention mekanizmalar\u0131 bir\u00e7ok state-of-the-art NLP modelinin bir par\u00e7as\u0131d\u0131r ve bu modellerin \u00e7e\u015fitli g\u00f6revlerde performans\u0131n\u0131 geli\u015ftirme konusunda \u00e7ok etkili oldu\u011fu g\u00f6sterilmi\u015ftir. ChatGPT sonu Aslinda konuya hakim biri icin daha iyi bir aciklama fakat cok kisa bu alanda hicbir bilgisi olmayan birisi icin basitlestirilmis bir aciklama burada harika bir sekilde anlatilmis Mechanics of Seq2seq Models With Attention Illustrated Transformer","title":"Attention Mekanizmasi"},{"location":"vit/#vit-implementasyonlari","text":"Fine-tune edilmis ve pre-trained ViT modelleri Google Research 'un Github'\u0131nda mevcut: https://github.com/google-research/vision_transformer Pytorch Implementasyonlari lucidrains'in Github reposunda bulabilirsiniz: https://github.com/lucidrains/vit-pytorch Ayni zamanda timm kullanarak hazir modelleri hizlica kullanabilirsiniz. https://github.com/rwightman/pytorch-image-models","title":"ViT Implementasyonlari"},{"location":"vit/#mimari","text":"ViT mimarisi birka\u00e7 a\u015famadan olu\u015fuyor: Patch + Position Embedding (inputs) - Giri\u015f g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc bir dizi g\u00f6r\u00fcnt\u00fc parcalarina (patches) d\u00f6n\u00fc\u015ft\u00fcr\u00fcr ve parcalarin hangi s\u0131rayla geldi\u011fini bilmek icin bir konum numaras\u0131 ekler. Linear projection of flattened patches (Embedded Patches) - G\u00f6r\u00fcnt\u00fc parcalari embeddinglere donusturulur, g\u00f6r\u00fcnt\u00fcleri direkt kullanmak yerine embeddingleri kullanman\u0131n yarar\u0131, embeddingler g\u00f6r\u00fcnt\u00fcn\u00fcn e\u011fitimle \u00f6\u011frenilebilir bir temsili olmas\u0131d\u0131r. Norm - Bir sinir a\u011f\u0131n\u0131 d\u00fczenli hale getirmek (overfitting'i azaltmak) i\u00e7in bir teknik olan \"Layer Normalization\" veya \"LayerNorm\"un k\u0131saltmas\u0131d\u0131r. Multi-Head Attention - Bu, Multi-Headed Self-Attention layer veya k\u0131saca \"MSA\" d\u0131r. MLP (Multilayer perceptron) - Genellikle herhangi bir feed-forward (ileri besleme) katman\u0131 koleksiyonu olarak dusunebilirsiniz. Transformer Encoder - Transformer Encoder, yukar\u0131da listelenen katmanlar\u0131n bir koleksiyonudur. Transformer Encoderin i\u00e7inde iki skip (atlama) ba\u011flant\u0131s\u0131 vard\u0131r (\"+\" sembolleri), katman\u0131n girdilerinin do\u011frudan sonraki katmanlar\u0131n yan\u0131 s\u0131ra hemen sonraki katmanlara beslendi\u011fi anlam\u0131na gelir. Genel ViT mimarisi, birbiri \u00fczerine y\u0131\u011f\u0131lm\u0131\u015f bir dizi Transformer kodlay\u0131c\u0131dan olu\u015fur. MLP Head - Bu, mimarinin \u00e7\u0131kt\u0131 katman\u0131d\u0131r, bir girdinin \u00f6\u011frenilen \u00f6zelliklerini bir s\u0131n\u0131f \u00e7\u0131kt\u0131s\u0131na d\u00f6n\u00fc\u015ft\u00fcr\u00fcr. G\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 \u00fczerinde \u00e7al\u0131\u015ft\u0131\u011f\u0131m\u0131z i\u00e7in buna \"s\u0131n\u0131fland\u0131r\u0131c\u0131 kafa\" da diyebilirsiniz. MLP head yap\u0131s\u0131 MLP blo\u011funa benzer.","title":"Mimari"},{"location":"vit/#vit-mimarisi","text":"","title":"ViT Mimarisi"},{"location":"vit/#patch-embeddings","text":"Standart Transformer, giri\u015fi tek boyutlu token embedding dizisi olarak al\u0131r. 2B g\u00f6r\u00fcnt\u00fcleri i\u015flemek i\u00e7in x\u2208R^{H\u00d7W\u00d7C} g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc d\u00fczle\u015ftirilmi\u015f 2B patchlere (goruntu parcalarina) yeniden \u015fekillendiriyoruz. Burada, (H, W) orijinal g\u00f6r\u00fcnt\u00fcn\u00fcn \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr ve (P, P) her g\u00f6r\u00fcnt\u00fc par\u00e7as\u0131n\u0131n \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr. Resim sabit boyutlu parcalara b\u00f6l\u00fcnm\u00fc\u015ft\u00fcr, a\u015fa\u011f\u0131daki resimde patch (parca) boyutu 16\u00d716 olarak al\u0131nm\u0131\u015ft\u0131r. Yani g\u00f6r\u00fcnt\u00fcn\u00fcn boyutlar\u0131 48\u00d748 olacakt\u0131r. (Cunku 3 kanal var) Self-attention maliyeti quadratictir. G\u00f6r\u00fcnt\u00fcn\u00fcn her pikselini girdi olarak iletirsek, Self-attention her pikselin di\u011fer t\u00fcm piksellerle ilgilenmesini gerektirir. Self-attention ikinci dereceden maliyeti \u00e7ok maliyetli olacak ve ger\u00e7ek\u00e7i girdi boyutuna \u00f6l\u00e7eklenmeyecek; bu nedenle, g\u00f6r\u00fcnt\u00fc parcalara b\u00f6l\u00fcn\u00fcr. Yani sair burada her pikselle ugrasmak sonsuza kadar surecegi icin 16x16 boyutlu goruntu bolumlerinin embeddinglerini almanin parametre sayisini dusureceginden bahsediyor. import matplotlib.pyplot as plt from PIL import Image import numpy as np img = Image.open('cobanov-profile.jpg') img.thumbnail((224, 224)) array_img = np.array(img) array_img.shape # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\ \\nNumber of patches per column: {num_patches}\\ \\nTotal patches: {num_patches*num_patches}\\ \\nPatch size: {patch_size} pixels x {patch_size} pixels\") # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float ncols=img_size // patch_size, figsize=(num_patches, num_patches), sharex=True, sharey=True) # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels)) axs[i, j].imshow(array_img[patch_height:patch_height+patch_size, # iterate through height patch_width:patch_width+patch_size, # iterate through width :]) # get all color channels # Set up label information, remove the ticks for clarity and set labels to outside axs[i, j].set_ylabel(i+1, rotation=\"horizontal\", horizontalalignment=\"right\", verticalalignment=\"center\") axs[i, j].set_xlabel(j+1) axs[i, j].set_xticks([]) axs[i, j].set_yticks([]) axs[i, j].label_outer() # Set a super title plt.show()","title":"Patch Embeddings"},{"location":"vit/#linear-projection-of-flattened-patches","text":"Parcalari Transformer blo\u011funa ge\u00e7irmeden \u00f6nce, makalenin yazarlar\u0131 yamalar\u0131 \u00f6nce do\u011frusal bir projeksiyondan ge\u00e7irmeyi faydal\u0131 bulmuslar. Bir yamay\u0131 al\u0131p b\u00fcy\u00fck bir vekt\u00f6re a\u00e7arlar ve patch embeddingler (goruntu parcalarinin gommeleri? veya embedddingleri) olu\u015fturmak i\u00e7in embedding matrisiyle \u00e7arparlar ve bu konumsal g\u00f6mmeyle (positional embeddings) birlikte transformat\u00f6re giden \u015feydir. Her goruntu parcasi (patches), t\u00fcm piksel kanallar\u0131n\u0131 bir yamada birle\u015ftirerek ve ard\u0131ndan bunu do\u011frusal olarak istenen giri\u015f boyutuna yans\u0131tarak embed edilen bir 1B patch'e d\u00fczle\u015ftirilir. Ne demek istedigimi bu gorselde cok daha iyi anlayacaginizi dusunuyorum. Kaynak: Face Transformer for Recognition","title":"Linear Projection of Flattened Patches"},{"location":"vit/#positional-embeddings","text":"Nasil konusurken dilde kelimelerin s\u0131ras\u0131 kurdugunuz cumlenin anlamini tamamen degistiriyorsa, goruntuler uzerinde de buna dikkat etmek gerekiyor. Maalesef transformerlar, patch embeddinglerin \"s\u0131ras\u0131n\u0131\" dikkate alan herhangi bir varsay\u0131lan mekanizmaya sahip de\u011filler. Bir yapboz yaptiginizi dusunun, elinizdeki parcalar (yani onceki adimlarda yaptigimiz patch embeddingler) karisik bir duzende geldiginde goruntunun tamaminda ne oldugunu anlamak oldukca zordur, bu transformerlar i\u00e7in de ge\u00e7erli. Modelin yapboz par\u00e7alar\u0131n\u0131n s\u0131ras\u0131n\u0131 veya konumunu \u00e7\u0131karmas\u0131n\u0131 sa\u011flaman\u0131n bir yoluna ihtiyac\u0131m\u0131z var. Transformerlar, giri\u015f elemanlar\u0131n\u0131n yap\u0131s\u0131ndan ba\u011f\u0131ms\u0131zd\u0131r. Her yamaya \u00f6\u011frenilebilir positional embeddings (konum yerle\u015ftirmeleri) eklemek, modelin g\u00f6r\u00fcnt\u00fcn\u00fcn yap\u0131s\u0131 hakk\u0131nda bilgi edinmesine olanak tan\u0131r. Positional embeddingler de, bu d\u00fczeni modele aktarmamizi sagliyor. ViT i\u00e7in, bu positional embeddingler, patch embeddingler ile ayn\u0131 boyutlulu\u011fa sahip \u00f6\u011frenilmi\u015f vekt\u00f6rlerdir. Bu positional embeddingler, e\u011fitim s\u0131ras\u0131nda ve (bazen) ince ayar s\u0131ras\u0131nda \u00f6\u011frenilir. E\u011fitim s\u0131ras\u0131nda, bu embeddingler, \u00f6zellikle ayn\u0131 s\u00fctunu ve sat\u0131r\u0131 payla\u015fan kom\u015fu konum yerle\u015ftirmelerine y\u00fcksek benzerlik g\u00f6sterdikleri vekt\u00f6r uzaylar\u0131nda birle\u015fir.","title":"Positional embeddings"},{"location":"vit/#transformer-encoding","text":"Multi-Head Self Attention Layer(MSP) birden fazla attention ciktisini lineer olarak beklenen boyutlara esitlemek i\u00e7in kullanilir. MSP, g\u00f6r\u00fcnt\u00fcdeki yerel ve global ba\u011f\u0131ml\u0131l\u0131klar\u0131 \u00f6\u011frenmeye yard\u0131mc\u0131 olur. Multi-Layer Perceptrons(MLP) - Klasik sinir agi katmani fakat aktivasyon fonksiyonu olarak GELU Gaussian Error Linear Units kullaniyoruz. Layer Norm(LN) e\u011fitim g\u00f6r\u00fcnt\u00fcleri aras\u0131nda herhangi bir yeni ba\u011f\u0131ml\u0131l\u0131k getirmedi\u011finden her bloktan \u00f6nce uygulan\u0131r. E\u011fitim s\u00fcresini ve genelleme performans\u0131n\u0131 iyile\u015ftirmeye yard\u0131mc\u0131 olur. Burada Misra'nin harika bir videosu var. Bu da Paper . Residual connections gradyanlar\u0131n do\u011frusal olmayan aktivasyonlardan ge\u00e7meden do\u011frudan a\u011f \u00fczerinden akmas\u0131na izin verdi\u011fi i\u00e7in her bloktan sonra uygulan\u0131r. Image classification i\u00e7in, \u00f6n e\u011fitim zaman\u0131nda bir hidden layer ve fine-tuning i\u00e7in tek bir linear layer ile MLP kullan\u0131larak bir classification head uygulan\u0131r. ViT'nin \u00fcst katmanlar\u0131 global \u00f6zellikleri \u00f6\u011frenirken, alt katmanlar hem global hem de yerel \u00f6zellikleri \u00f6\u011frenir. Bu da aslinda ViT'nin daha genel kal\u0131plar\u0131 \u00f6\u011frenmesini sagliyor.","title":"Transformer Encoding"},{"location":"vit/#toparlama","text":"Evet oldukca fazla terim, teori ve mimari inceledik kafalarda daha iyi oturtmak adina bu gif surecin nasil isledigini guzel bir sekilde ozetliyor.","title":"Toparlama"},{"location":"vit/#usage","text":"Eger basitce bir ViT kullanmak isterseniz bunun icin ufacik bir rehberi de buraya ekliyorum. Muhtemelen denk gelmissinizdir artik yapay zeka alaninda yeni bir sey ciktiginda birakin bunun implementasyonunu kullanimini da birkac satir koda indirmek moda oldu. Yukarida anlattigim her seyi birkac satir Python koduyla nasil yapildigina bakalim. Colab linki: https://colab.research.google.com/drive/1sPafxIo6s1BBjHbl9e0b_DYGlb2AMBC3?usp=sharing Oncelikle pretrained model instantiate (orneklendirmek?) edelim. import timm model = timm.create_model('vit_base_patch16_224', pretrained=True) model.eval() Goruntumuzu yukleyip on islemelerini tamamlayim. Ben burada twitter profil fotograimi kullanacagim. # eger kendiniz bir gorsel vermek isterseniz # asagidaki kodda bu kismi comment'e alip, # filename kismina local path verebilirsiniz. url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\") urllib.request.urlretrieve(url, filename) import urllib from PIL import Image from timm.data import resolve_data_config from timm.data.transforms_factory import create_transform config = resolve_data_config({}, model=model) transform = create_transform(**config) url, filename = (\"https://pbs.twimg.com/profile_images/1594739904642154497/-7kZ3Sf3_400x400.jpg\", \"mert.jpg\") urllib.request.urlretrieve(url, filename) img = Image.open(filename).convert('RGB') tensor = transform(img).unsqueeze(0) # transform and add batch dimension Tahminleri alalim import torch with torch.no_grad(): out = model(tensor) probabilities = torch.nn.functional.softmax(out[0], dim=0) print(probabilities.shape) # prints: torch.Size([1000]) En populer 5 tahminin siniflarina bakalim. # Get imagenet class mappings url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\") urllib.request.urlretrieve(url, filename) with open(\"imagenet_classes.txt\", \"r\") as f: categories = [s.strip() for s in f.readlines()] # Print top categories per image top5_prob, top5_catid = torch.topk(probabilities, 5) for i in range(top5_prob.size(0)): print(categories[top5_catid[i]], top5_prob[i].item()) trench coat 0.422695130109787 bulletproof vest 0.18995067477226257 suit 0.06873432546854019 sunglasses 0.02222270704805851 sunglass 0.020680639892816544","title":"Usage"},{"location":"vit/#sources","text":"https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview https://theaisummer.com/vision-transformer/ https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2 https://viso.ai/deep-learning/vision-transformer-vit/ https://arxiv.org/abs/2106.01548","title":"Sources"}]}